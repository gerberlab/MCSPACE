{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d259ba",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "- common interface for each type of model, same trainer and streamline of result output/analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78def0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mcspace.visualization as vis\n",
    "from mcspace.dataset import PerturbationDataSet\n",
    "from mcspace.utils import pickle_load\n",
    "from pathlib import Path\n",
    "from mcspace.models import PerturbationModel\n",
    "import torch\n",
    "from mcspace.data_utils import get_normed_data_garb_clusters\n",
    "from mcspace.trainer import train\n",
    "# TODO: add model and vis of model reults; as well as post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b07154d",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe4658fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is 7CF0-0838\n",
      "\n",
      " Directory of C:\\Users\\guppa\\Dropbox (Partners HealthCare)\\research_bwh\\mapseq_topic_model_JULY_2022\\MCSPACE_model\\mcspace\\data\\FMT_MAPSEQ_data\\processed_data\n",
      "\n",
      "08/05/2023  06:46 PM    <DIR>          .\n",
      "08/05/2023  02:55 PM    <DIR>          ..\n",
      "08/05/2023  02:49 PM         7,652,385 env_data.tsv\n",
      "08/05/2023  02:54 PM        14,639,349 env2jax_data.tsv\n",
      "08/05/2023  06:46 PM        34,620,160 filtered_dataset.pkl\n",
      "08/05/2023  02:47 PM         7,331,013 jax_data.tsv\n",
      "               4 File(s)     64,242,907 bytes\n",
      "               2 Dir(s)  47,192,596,480 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls \"../data/FMT_MAPSEQ_data/processed_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce71a13d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datapath = Path(\"../data/FMT_MAPSEQ_data/processed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba7ce516",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle_load(datapath / \"filtered_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c80135e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 groups: pre-perturb, post-perturb, comparator\n",
      "165 OTUs in study\n",
      "4 subjects per group\n",
      "stats for pre_perturb group:\n",
      "\t Subject J1:\n",
      "\t\t 353 particles\n",
      "\t\t min read depth: 2218\n",
      "\t\t median read depth: 4245.0\n",
      "\t\t max read depth: 122311\n",
      "\t Subject J2:\n",
      "\t\t 353 particles\n",
      "\t\t min read depth: 2396\n",
      "\t\t median read depth: 4781.0\n",
      "\t\t max read depth: 678320\n",
      "\t Subject J3:\n",
      "\t\t 336 particles\n",
      "\t\t min read depth: 1734\n",
      "\t\t median read depth: 2980.0\n",
      "\t\t max read depth: 242450\n",
      "\t Subject J4:\n",
      "\t\t 229 particles\n",
      "\t\t min read depth: 777\n",
      "\t\t median read depth: 1250.0\n",
      "\t\t max read depth: 245585\n",
      "\t 1271 particles for group pre_perturb\n",
      "\n",
      "\n",
      "stats for post_perturb group:\n",
      "\t Subject JE10:\n",
      "\t\t 417 particles\n",
      "\t\t min read depth: 1416\n",
      "\t\t median read depth: 2903.0\n",
      "\t\t max read depth: 77217\n",
      "\t Subject JE11:\n",
      "\t\t 384 particles\n",
      "\t\t min read depth: 1246\n",
      "\t\t median read depth: 2842.0\n",
      "\t\t max read depth: 71299\n",
      "\t Subject JE12:\n",
      "\t\t 344 particles\n",
      "\t\t min read depth: 1434\n",
      "\t\t median read depth: 3229.0\n",
      "\t\t max read depth: 273209\n",
      "\t Subject JE9:\n",
      "\t\t 321 particles\n",
      "\t\t min read depth: 1035\n",
      "\t\t median read depth: 2653.0\n",
      "\t\t max read depth: 204436\n",
      "\t 1466 particles for group post_perturb\n",
      "\n",
      "\n",
      "stats for comparator group:\n",
      "\t Subject E5:\n",
      "\t\t 606 particles\n",
      "\t\t min read depth: 605\n",
      "\t\t median read depth: 1187.5\n",
      "\t\t max read depth: 23658\n",
      "\t Subject E6:\n",
      "\t\t 470 particles\n",
      "\t\t min read depth: 930\n",
      "\t\t median read depth: 1837.5\n",
      "\t\t max read depth: 65040\n",
      "\t Subject E7:\n",
      "\t\t 437 particles\n",
      "\t\t min read depth: 1014\n",
      "\t\t median read depth: 1898.0\n",
      "\t\t max read depth: 65452\n",
      "\t Subject E8:\n",
      "\t\t 443 particles\n",
      "\t\t min read depth: 1337\n",
      "\t\t median read depth: 2516.0\n",
      "\t\t max read depth: 72939\n",
      "\t 1956 particles for group comparator\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86dff7",
   "metadata": {},
   "source": [
    "# Load trainer and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96dd991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87fe0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get from dataset\n",
    "num_otus = 165\n",
    "num_subjects = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb5da4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put in utils, only have dataset as input\n",
    "def estimate_group_variances(data, notus):\n",
    "    groups = data.keys()\n",
    "    xvar = {}\n",
    "\n",
    "    for grp in groups:\n",
    "        grpdata = data[grp]\n",
    "        nsubj = len(list(grpdata.keys()))\n",
    "        sdata = np.zeros((notus, nsubj))\n",
    "        for i,s in enumerate(grpdata.keys()):\n",
    "            counts = grpdata[s]\n",
    "            ra = counts.sum(axis=0)/counts.sum()\n",
    "            sdata[:,i] = np.log(ra + 1e-20)\n",
    "\n",
    "        #* see outliers > 50; filter out and take median\n",
    "        if nsubj < 3:\n",
    "            svarmed = 0.1\n",
    "        else:\n",
    "            svar = np.var(sdata, axis=1)\n",
    "            svarmed = np.median(svar)\n",
    "        xvar[grp] = svarmed\n",
    "    return xvar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08831e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf3e8fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_variance = estimate_group_variances(data, num_otus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f23b393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bba762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cde238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a90fa669",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") #cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "465ad495",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datatrain, garbclusts = get_normed_data_garb_clusters(data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78888f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4f32c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PerturbationModel(num_communities=20,\n",
    "                         num_otus=num_otus,\n",
    "                         num_subjects=num_subjects,\n",
    "                         subject_variance=subject_variance,\n",
    "                         device=torch.device('cpu')\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ca2111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a3ec18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0d2cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(reads_in, device):\n",
    "    reads = torch.from_numpy(reads_in).to(torch.float)\n",
    "    norm = torch.sum(reads, dim=1)\n",
    "    rel_data = torch.div(reads, norm.unsqueeze(1))\n",
    "    z_data = torch.log(rel_data+0.0001)\n",
    "    z_std, z_mean = torch.std_mean(z_data, dim=1)\n",
    "    z_data = z_data - z_mean.unsqueeze(1)\n",
    "    z_data = torch.div(z_data, z_std.unsqueeze(1))\n",
    "\n",
    "    if z_data.isnan().any():\n",
    "        raise ValueError(\"nan in normed data\")\n",
    "\n",
    "    return {'count_data': reads.to(device), 'normed_data': z_data.to(device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8674d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normed_reads_counts_combine_reps_group_garb(reads, device):\n",
    "    # reads is a dict [t] of dicts [s]\n",
    "    counts = {}\n",
    "    garbage_clusters = {}\n",
    "    normed_data = {}\n",
    "    full_normed_data = [] #* output L* x O; for all particles concatenated together...\n",
    "\n",
    "    for t in reads.keys():\n",
    "        subjs = reads[t].keys()\n",
    "        counts[t] = {}\n",
    "        all_particles = None\n",
    "        # garbage_clusters[t] = {}\n",
    "        normed_data[t] = {}\n",
    "        for s in subjs:\n",
    "            reps = reads[t][s].keys()\n",
    "            combined_reads = reads[t][s]\n",
    "            counts[t][s] = torch.from_numpy(reads[t][s]).to(dtype=torch.float, device=device)\n",
    "            data = get_data(combined_reads, device)\n",
    "            normed_data[t][s] = data['normed_data']\n",
    "            full_normed_data.append(data['normed_data'])\n",
    "\n",
    "        bulk = all_particles.sum(axis=0)/all_particles.sum()\n",
    "        garbage_clusters[t] = torch.from_numpy(bulk).to(dtype=torch.float, device=device)\n",
    "    combined_data = torch.cat(full_normed_data, dim=0)\n",
    "    return {'count_data': counts, 'normed_data': normed_data, 'full_normed_data': combined_data}, garbage_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1559873b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28263bee",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d81af71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mcspace.dataset.PerturbationDataSet at 0x2a96774d460>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4778e650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53e474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8267d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "576d0d3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PerturbationDataSet' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\guppa\\dropbox (partners healthcare)\\research_bwh\\mapseq_topic_model_july_2022\\mcspace_model\\mcspace\\trainer.py:18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data, num_epochs, verbose)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, num_epochs):\n\u001b[0;32m     17\u001b[0m     model\u001b[38;5;241m.\u001b[39mcommunity_distribution\u001b[38;5;241m.\u001b[39mset_temps(epoch, num_epochs)\n\u001b[1;32m---> 18\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[0;32m     20\u001b[0m     model\u001b[38;5;241m.\u001b[39mELBO_loss\u001b[38;5;241m.\u001b[39mbackward() \n",
      "File \u001b[1;32mc:\\users\\guppa\\dropbox (partners healthcare)\\research_bwh\\mapseq_topic_model_july_2022\\mcspace_model\\mcspace\\models.py:240\u001b[0m, in \u001b[0;36mPerturbationModel.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# comm_distrib.shape = k x g x s \u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     comm_distrib, KL_comm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunity_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misfinite(comm_distrib)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan or inf in community distribution\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mcspace\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\guppa\\dropbox (partners healthcare)\\research_bwh\\mapseq_topic_model_july_2022\\mcspace_model\\mcspace\\community_distributions.py:604\u001b[0m, in \u001b[0;36mPerturbationCommunityDistribution.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;66;03m#* input is normed data; no counts ; what about 'full data'; over all particles, for all samples \u001b[39;00m\n\u001b[1;32m--> 604\u001b[0m     normed_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnormed_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    605\u001b[0m     full_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_normed_data\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# should be Lfull x O array\u001b[39;00m\n\u001b[0;32m    607\u001b[0m     x_jax, KL_jax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_jax(normed_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_perturb\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'PerturbationDataSet' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "losses = train(model, dataset, num_epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb03f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8069d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "661ec3a3",
   "metadata": {},
   "source": [
    "# Post process runs, evaulate stability metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a2687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no stability metric; just need bayes factors!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe946e75",
   "metadata": {},
   "source": [
    "# Create figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3674e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# would like tree methods to work here too...\n",
    "# could use a 'preprocessed tree' -- we're not giving a tutorial on pplacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d009f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mcspace]",
   "language": "python",
   "name": "conda-env-mcspace-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
